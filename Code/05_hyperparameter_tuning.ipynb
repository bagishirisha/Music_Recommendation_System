{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66fd37e-6415-4ef5-88a4-f8f4ede27df0",
   "metadata": {},
   "source": [
    "## Step 05: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b776fba-f31d-4e5f-aa6e-a6e5094117d0",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b88b99d-ceca-412c-b434-0265d24699b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score,calinski_harabasz_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0eddca-4c08-4341-ae02-b2a8b89fa23e",
   "metadata": {},
   "source": [
    "### 5.1 Load datasets from file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6723b340-a460-4e99-9dec-e6a4f563842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Data and genre data successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "data_path = '..\\Dataset\\data_cleaned.csv'\n",
    "genre_data_path = '..\\Dataset\\genre_data_cleaned.csv'\n",
    "\n",
    "# Check if files exist and load them\n",
    "if os.path.exists(data_path) and os.path.exists(genre_data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    genre_data = pd.read_csv(genre_data_path)\n",
    "    print(\"Info: Data and genre data successfully loaded.\")\n",
    "else:\n",
    "    print(\"Attention: One or both files are not found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "737cb614-af97-4132-b364-d968c5abbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = genre_data.select_dtypes(include=np.number) \n",
    "Y = data.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0059690-9476-4d8c-86b2-a0097cf88e88",
   "metadata": {},
   "source": [
    "### 5.2  KMeans Hyperparameter tuning \n",
    "- Exhaustive grid search is perfomed over KMeans hyperparameters using a training-validation split of 80%-20%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dec85-7774-4a76-a66b-5adcccae249d",
   "metadata": {},
   "source": [
    "#### 5.2.1 Genre_data_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e9f9b-d8d0-471c-b1e4-017424bcd5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Splitting the dataset \n",
      "\n",
      "Training set shape: (2378, 13)\n",
      "Test set shape: (595, 13)\n",
      "\n",
      "Step 2: Defining the parameter grid for KMeans...\n",
      "\n",
      "Total parameter combinations to evaluate: 32\n",
      "\n",
      "Step 3: Setting up tracking for the best and worst parameters...\n",
      "\n",
      "Step 4: Setting up 3-Fold Cross-Validation...\n",
      "\n",
      "Step 5: Starting the grid search...\n",
      "\n",
      "ğŸŒ€ Iteration 1/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2593\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9706\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.5352\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2538\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9432\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 364.7971\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2634\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9484 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 376.6187 (higher is better)\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 2/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2594\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9696\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.4925\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2651\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9310\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 368.1921\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2672\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9440 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 377.7361 (higher is better)\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 3/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2593\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9706\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.5352\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2777\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9267\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 394.4946\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2714\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9429 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 386.5178 (higher is better)\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 4/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2594\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9696\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.4925\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2671\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9413\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 376.0270\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2679\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9474 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 380.3477 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 5/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2228\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0363\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 325.0101\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2490\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9630\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 360.0244\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2370\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9984 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 347.5895 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 6/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2228\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0363\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 325.0101\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2503\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9643\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 360.1193\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2375\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9988 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 347.6212 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 7/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2441\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9897\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 343.6520\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2620\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9234\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 379.0954\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2485\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9696 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 360.1605 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 8/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2441\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9897\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 343.6520\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2627\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9239\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 378.6631\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2487\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9698 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 360.0164 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 9/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2593\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9706\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.5352\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2538\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9432\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 364.7971\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2634\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9484 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 376.6187 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 10/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2594\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9696\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.4925\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2651\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9310\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 368.1921\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2672\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9440 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 377.7361 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 11/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2593\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9706\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.5352\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2777\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9267\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 394.4946\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2714\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9429 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 386.5178 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 12/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2594\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9696\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 363.4925\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2671\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9413\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 376.0270\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2772\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9313\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 401.5237\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2679\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9474 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 380.3477 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 13/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2228\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0363\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 325.0101\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2490\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9630\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 360.0244\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2370\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9984 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 347.5895 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 14/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2228\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0363\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 325.0101\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2503\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9643\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 360.1193\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2375\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9988 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 347.6212 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 15/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2441\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9897\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 343.6520\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2620\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9234\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 379.0954\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2485\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9696 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 360.1605 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 16/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2441\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9897\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 343.6520\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2627\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9239\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 378.6631\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9958\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 357.7340\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2487\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9698 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 360.0164 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 17/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2556\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9543\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 368.8348\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2793\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9367\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.0860\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2657\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9514 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 380.1995 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 18/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2564\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9544\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 368.8502\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2801\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9345\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.4930\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2662\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9507 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 380.3404 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 19/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2720\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9356\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 378.6082\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2793\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9367\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.0860\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2711\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9452 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 383.4573 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 20/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2685\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9324\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 378.0177\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2801\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9345\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.4930\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2702\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9434 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 383.3962 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 21/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2256\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0270\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 339.0570\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2293\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0098\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 342.8937\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2544\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9504\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.1939\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2365\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9957 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 350.0482 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 22/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2255\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0277\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 339.0236\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2298\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0088\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 343.4075\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2548\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9509\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.2701\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2367\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9958 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 350.2337 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 23/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2230\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0476\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 330.7985\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2478\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9838\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 353.3592\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2544\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9504\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.1939\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2417\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9939 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 350.7839 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 24/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9940\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 343.2427\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2588\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9627\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 356.5368\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2548\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9509\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.2701\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2510\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9692 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 356.0166 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 25/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2556\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9543\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 368.8348\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2793\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9367\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.0860\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2657\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9514 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 380.1995 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 26/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2564\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9544\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 368.8502\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2801\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9345\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.4930\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2662\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9507 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 380.3404 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 27/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2720\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9356\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 378.6082\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2793\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9367\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.0860\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2711\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9452 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 383.4573 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 28/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2622\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9633\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 367.6779\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2685\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9324\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 378.0177\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2801\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9345\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 404.4930\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2702\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9434 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 383.3962 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 29/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2256\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0270\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 339.0570\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2293\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0098\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 342.8937\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2544\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9504\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.1939\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2365\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9957 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 350.0482 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 30/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2255\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0277\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 339.0236\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2298\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0088\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 343.4075\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2548\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9509\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.2701\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2367\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9958 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 350.2337 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 31/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2230\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0476\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 330.7985\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2478\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9838\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 353.3592\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2544\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9504\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.1939\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2417\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9939 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 350.7839 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 32/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2394\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9940\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 343.2427\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2588\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9627\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 356.5368\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2548\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9509\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 368.2701\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2510\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9692 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 356.0166 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ‰ Grid Search Completed!\n",
      "ğŸ” Best Parameters for KMeans: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}\n",
      "ğŸ“ˆ Best Silhouette Score (Validation): 0.2714\n",
      "ğŸ“Š Best Davies-Bouldin Index (Validation): 0.9692 (lower is better)\n",
      "ğŸ“Š Best Calinski-Harabasz Score (Validation): 356.0166 (higher is better)\n",
      "\n",
      "ğŸ” Worst Parameters for KMeans: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}\n",
      "ğŸ“‰ Worst Silhouette Score (Validation): 0.2365\n",
      "\n",
      "\n",
      "ğŸ” Testing on unseen data with the best parameters\n",
      "âœ… Silhouette Score (Testing): 0.2846\n",
      "âœ… Davies-Bouldin Index  (Testing):  0.9270\n",
      "âœ… Calinski-Harabasz Score  (Testing):  313.9262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert dataset to NumPy array and split into training/validation sets\n",
    "print(\"Step 1: Splitting the dataset \\n\")\n",
    "X_np = X.to_numpy()\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test = train_test_split(X_np, test_size=0.2, random_state=42)\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\\n\")\n",
    "\n",
    "# Step 2: Define parameter grid for KMeans\n",
    "print(\"Step 2: Defining the parameter grid for KMeans...\\n\")\n",
    "param_grid_kmeans = {\n",
    "    'n_clusters': [13, 20],\n",
    "    'init': ['k-means++', 'random'],\n",
    "    'n_init': [10, 20],\n",
    "    'max_iter': [300, 500],\n",
    "    'tol': [1e-3, 1e-4],\n",
    "}\n",
    "\n",
    "parameter_combinations = list(ParameterGrid(param_grid_kmeans))\n",
    "total_iterations = len(parameter_combinations)\n",
    "print(f\"Total parameter combinations to evaluate: {total_iterations}\\n\")\n",
    "\n",
    "# Step 3: Initialize tracking for the best and worst parameters\n",
    "print(\"Step 3: Setting up tracking for the best and worst parameters...\\n\")\n",
    "best_score = -1\n",
    "worst_score = np.inf\n",
    "best_params = None\n",
    "worst_params = None\n",
    "\n",
    "# Step 4: Set up K-Fold cross-validation\n",
    "print(\"Step 4: Setting up 3-Fold Cross-Validation...\\n\")\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 5: Start the grid search\n",
    "print(\"Step 5: Starting the grid search...\\n\")\n",
    "current_iteration = 0\n",
    "\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"ğŸŒ€ Iteration {current_iteration}/{total_iterations}: Testing parameters {params}...\\n\")\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    calinski_harabasz_scores = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):\n",
    "        print(f\"  â¡ï¸ Fold {fold}: Splitting data...\")\n",
    "        X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "\n",
    "        try:\n",
    "            # Initialize and fit KMeans\n",
    "            print(f\"    ğŸ”„ Running KMeans with parameters: {params}...\")\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=params['n_clusters'],\n",
    "                init=params['init'],\n",
    "                n_init=params['n_init'],\n",
    "                max_iter=params['max_iter'],\n",
    "                tol=params['tol'],\n",
    "                random_state=42\n",
    "            )\n",
    "            kmeans.fit(X_train_cv)\n",
    "\n",
    "            # Predict on the validation fold\n",
    "            labels_val = kmeans.predict(X_val_cv)\n",
    "\n",
    "            # Compute metrics\n",
    "            silhouette_val = silhouette_score(X_val_cv, labels_val)\n",
    "            dbi_val = davies_bouldin_score(X_val_cv, labels_val)\n",
    "            ch_score_val = calinski_harabasz_score(X_val_cv, labels_val)\n",
    "\n",
    "            # Append scores\n",
    "            silhouette_scores.append(silhouette_val)\n",
    "            davies_bouldin_scores.append(dbi_val)\n",
    "            calinski_harabasz_scores.append(ch_score_val)\n",
    "\n",
    "            print(f\"    âœ… Silhouette Score for Fold {fold}: {silhouette_val:.4f}\")\n",
    "            print(f\"    âœ… Davies-Bouldin Index for Fold {fold}: {dbi_val:.4f}\")\n",
    "            print(f\"    âœ… Calinski-Harabasz Score for Fold {fold}: {ch_score_val:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Error during Fold {fold}: {e}\")\n",
    "            silhouette_scores.append(np.nan)\n",
    "            davies_bouldin_scores.append(np.nan)\n",
    "            calinski_harabasz_scores.append(np.nan)\n",
    "\n",
    "    # Compute average scores for this parameter set\n",
    "    mean_silhouette = np.nanmean(silhouette_scores)\n",
    "    mean_dbi = np.nanmean(davies_bouldin_scores)\n",
    "    mean_ch_score = np.nanmean(calinski_harabasz_scores)\n",
    "\n",
    "    print(f\"  ğŸ“Š Average Silhouette Score: {mean_silhouette:.4f}\")\n",
    "    print(f\"  ğŸ“Š Average Davies-Bouldin Index: {mean_dbi:.4f} (lower is better)\")\n",
    "    print(f\"  ğŸ“Š Average Calinski-Harabasz Score: {mean_ch_score:.4f} (higher is better)\\n\")\n",
    "\n",
    "    # Update the best parameters if a higher Silhouette Score is found\n",
    "    if mean_silhouette > best_score:\n",
    "        print(\"  ğŸ‰ New best parameters found!\")\n",
    "        best_score = mean_silhouette\n",
    "        best_params = params\n",
    "\n",
    "    # Update the worst parameters if a lower Silhouette Score is found\n",
    "    if mean_silhouette < worst_score:\n",
    "        worst_score = mean_silhouette\n",
    "        worst_params = params\n",
    "\n",
    "    print(\"-\" * 50)  \n",
    "\n",
    "# Step 6: Print the summary of results\n",
    "print(\"\\nğŸ‰ Grid Search Completed!\")\n",
    "print(f\"ğŸ” Best Parameters for KMeans: {best_params}\")\n",
    "print(f\"ğŸ“ˆ Best Silhouette Score (Validation): {best_score:.4f}\")\n",
    "print(f\"ğŸ“Š Best Davies-Bouldin Index (Validation): {mean_dbi:.4f} (lower is better)\")\n",
    "print(f\"ğŸ“Š Best Calinski-Harabasz Score (Validation): {mean_ch_score:.4f} (higher is better)\\n\")\n",
    "print(f\"ğŸ” Worst Parameters for KMeans: {worst_params}\")\n",
    "print(f\"ğŸ“‰ Worst Silhouette Score (Validation): {worst_score:.4f}\\n\")\n",
    "\n",
    "# Step 7:Testing\n",
    "print(\"\\n Step 7: Evaluating on test set...\")\n",
    "final_kmeans = KMeans(\n",
    "    n_clusters=best_params['n_clusters'],\n",
    "    init=best_params['init'],\n",
    "    n_init=best_params['n_init'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    tol=best_params['tol'],\n",
    "    random_state=42\n",
    ")\n",
    "final_kmeans.fit(X_train)\n",
    "\n",
    "# Predict on test data\n",
    "labels_test = final_kmeans.predict(X_test)\n",
    "\n",
    "# Evaluate metrics on test data\n",
    "silhouette_test = silhouette_score(X_test, labels_test)\n",
    "dbi_test = davies_bouldin_score(X_test, labels_test)\n",
    "ch_score_test = calinski_harabasz_score(X_test, labels_test)\n",
    "\n",
    "print(f\"âœ… Silhouette Score (Testing): {silhouette_test:.4f}\")\n",
    "print(f\"âœ… Davies-Bouldin Index  (Testing):  {dbi_test:.4f}\")\n",
    "print(f\"âœ… Calinski-Harabasz Score  (Testing):  {ch_score_test:.4f}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f936e0",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "1. **Best Parameters Achieved**:\n",
    "   - **Initialization (`init`)**: `'k-means++'` performed better than `'random'` by producing well-spread initial cluster centers.\n",
    "   - **Number of Clusters (`n_clusters`)**: Using 13 clusters provided the best clustering quality.\n",
    "   - **Number of Initializations (`n_init`)**: 20 initializations improved the solution's quality and consistency.\n",
    "   - **Maximum Iterations (`max_iter`)**: 300 iterations were sufficient for convergence.\n",
    "   - **Tolerance (`tol`)**: A value of 0.001 ensured stable convergence.\n",
    "   - **Best Silhouette Score**: 0.2714, indicating moderately well-separated clusters.\n",
    "\n",
    "2. **Worst Parameters Observed**:\n",
    "   - **Initialization (`init`)**: `'random'` produced poor initial cluster centers, leading to suboptimal results.\n",
    "   - **Number of Clusters (`n_clusters`)**: Using 20 clusters resulted in over-segmentation and lower silhouette scores.\n",
    "   - **Worst Silhouette Score**: 0.2365, reflecting poor clustering structure.\n",
    "\n",
    "3. **Key Findings**:\n",
    "   - **Impact of Initialization**: `'k-means++'` consistently outperformed `'random'` initialization by creating better-defined clusters.\n",
    "   - **Optimal Number of Clusters**: Selecting too many clusters (e.g., 20) caused clusters to overlap or become poorly defined.\n",
    "   - **Importance of Multiple Initializations (`n_init`)**: Increasing `n_init` to 20 significantly improved clustering results by avoiding poor local minima.\n",
    "   - **Effect of Iterations and Tolerance**: Both `max_iter=300` and `tol=0.001` were sufficient to reach stable and reliable results.\n",
    "\n",
    "4. **Conclusion**:\n",
    "   - The best results were achieved with `'k-means++'` initialization, 13 clusters, and 20 initializations. Poor initialization and excessive clusters negatively impacted performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a117868-42e8-4180-bf8d-5d5c8470c8d4",
   "metadata": {},
   "source": [
    "#### 5.2.2 Data_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c282c-3c42-4f7d-bc4b-e3b831eb4ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Splitting the dataset \n",
      "\n",
      "Training set shape: (136522, 17)\n",
      "Test set shape: (34131, 17)\n",
      "\n",
      "Step 2: Defining the parameter grid for KMeans...\n",
      "\n",
      "Total parameter combinations to evaluate: 32\n",
      "\n",
      "Step 3: Setting up tracking for the best and worst parameters...\n",
      "\n",
      "Step 4: Setting up 3-Fold Cross-Validation...\n",
      "\n",
      "Step 5: Starting the grid search...\n",
      "\n",
      "ğŸŒ€ Iteration 1/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9934\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27813.1467\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2907\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9747\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27987.6801\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2914\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9735\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27970.6629\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2892\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9805 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27923.8299 (higher is better)\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 2/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2856\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9932\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27819.5243\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2910\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9739\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27989.4833\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2916\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9736\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27972.0099\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2894\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9802 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27927.0058 (higher is better)\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 3/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9937\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27812.8599\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2907\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9747\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27987.6801\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2871\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9923\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27933.1411\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2877\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9869 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27911.2270 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 4/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2856\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9931\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27819.5427\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2910\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9739\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27989.4833\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2872\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9909\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27933.2897\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2879\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9860 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27914.1052 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 5/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2683\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0553\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25381.7456\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2665\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0566\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25631.4954\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0769\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.4280\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2684\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0630 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25558.5563 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 6/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2686\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0754\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25511.3088\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2675\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0486\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25604.5140\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0766\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.1418\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2688\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0669 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25592.6549 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 7/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2633\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.1037\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25510.3172\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2630\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.1077\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25598.3618\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0769\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.4280\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2655\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0961 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25590.3690 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 8/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2686\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0754\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25511.3088\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2691\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0792\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25652.5148\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0766\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.1418\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2693\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0771 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25608.6551 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 9/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9934\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27813.1467\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2907\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9747\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27987.6801\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2914\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9735\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27970.6629\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2892\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9805 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27923.8299 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 10/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2856\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9932\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27819.5243\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2910\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9739\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27989.4833\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2916\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9736\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27972.0099\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2894\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9802 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27927.0058 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 11/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9937\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27812.8599\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2907\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9747\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27987.6801\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2871\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9923\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27933.1411\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2877\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9869 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27911.2270 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 12/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2856\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9931\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27819.5427\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2910\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9739\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27989.4833\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2872\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9909\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27933.2897\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2879\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9860 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27914.1052 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 13/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2683\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0553\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25381.7456\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2665\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0566\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25631.4954\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0769\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.4280\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2684\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0630 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25558.5563 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 14/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2686\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0754\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25511.3088\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2675\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0486\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25604.5140\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0766\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.1418\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2688\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0669 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25592.6549 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 15/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2633\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.1037\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25510.3172\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2630\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.1077\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25598.3618\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0769\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.4280\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2655\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0961 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25590.3690 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 16/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2686\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0754\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25511.3088\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2691\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0792\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25652.5148\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2703\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0766\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25662.1418\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2693\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0771 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25608.6551 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 17/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9916\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27814.2360\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2870\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9865\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 28008.1530\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2919\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9735\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27973.4922\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2881\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9839 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27931.9604 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 18/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2856\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9916\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27819.4178\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2909\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9743\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27986.9296\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2916\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9738\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27972.5543\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2894\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9799 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27926.3006 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 19/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9916\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27814.2360\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2905\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9757\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27976.7904\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2913\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9745\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27965.5893\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2891\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9806 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27918.8719 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 20/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2855\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9923\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27820.4395\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2909\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9744\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27986.9352\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2916\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9738\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27972.5543\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2893\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9801 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27926.6430 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 21/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2649\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0642\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25456.6700\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2658\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0631\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25633.0181\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2693\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0793\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25636.2987\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2666\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0688 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25575.3289 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 22/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2650\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0880\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25499.8995\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2656\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0629\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25628.6756\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2698\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0770\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25647.1370\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2668\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0760 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25591.9040 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 23/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2687\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0765\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25514.8183\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2632\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0987\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25635.0018\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2693\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0793\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25636.2987\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2670\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0848 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25595.3729 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 24/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2686\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0757\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25511.8974\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2690\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0793\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25655.5759\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2698\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0770\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25647.1370\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2691\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0773 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25604.8701 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 25/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9916\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27814.2360\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2870\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9865\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 28008.1530\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2919\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9735\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27973.4922\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2881\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9839 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27931.9604 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 26/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2856\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9916\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27819.4178\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2909\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9743\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27986.9296\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2916\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9738\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27972.5543\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2894\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9799 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27926.3006 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 27/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2854\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9916\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27814.2360\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2905\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9757\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27976.7904\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2913\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9745\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27965.5893\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2891\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9806 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27918.8719 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 28/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2855\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 0.9923\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 27820.4395\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2909\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 0.9744\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 27986.9352\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 11, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2916\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 0.9738\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 27972.5543\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2893\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 0.9801 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 27926.6430 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 29/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2649\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0642\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25456.6700\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2658\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0631\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25633.0181\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2693\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0793\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25636.2987\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2666\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0688 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25575.3289 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 30/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2650\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0880\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25499.8995\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2656\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0629\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25628.6756\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 10, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2698\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0770\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25647.1370\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2668\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0760 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25591.9040 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 31/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2687\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0765\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25514.8183\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2632\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0987\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25635.0018\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2693\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0793\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25636.2987\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2670\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0848 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25595.3729 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 32/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 1: 0.2686\n",
      "    âœ… Davies-Bouldin Index for Fold 1: 1.0757\n",
      "    âœ… Calinski-Harabasz Score for Fold 1: 25511.8974\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 2: 0.2690\n",
      "    âœ… Davies-Bouldin Index for Fold 2: 1.0793\n",
      "    âœ… Calinski-Harabasz Score for Fold 2: 25655.5759\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running KMeans with parameters: {'init': 'random', 'max_iter': 500, 'n_clusters': 14, 'n_init': 20, 'tol': 0.0001}...\n",
      "    âœ… Silhouette Score for Fold 3: 0.2698\n",
      "    âœ… Davies-Bouldin Index for Fold 3: 1.0770\n",
      "    âœ… Calinski-Harabasz Score for Fold 3: 25647.1370\n",
      "\n",
      "  ğŸ“Š Average Silhouette Score: 0.2691\n",
      "  ğŸ“Š Average Davies-Bouldin Index: 1.0773 (lower is better)\n",
      "  ğŸ“Š Average Calinski-Harabasz Score: 25604.8701 (higher is better)\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ‰ Grid Search Completed!\n",
      "ğŸ” Best Parameters for KMeans: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 11, 'n_init': 10, 'tol': 0.0001}\n",
      "ğŸ“ˆ Best Silhouette Score (Validation): 0.2894\n",
      "ğŸ“Š Best Davies-Bouldin Index (Validation): 1.0773 (lower is better)\n",
      "ğŸ“Š Best Calinski-Harabasz Score (Validation): 25604.8701 (higher is better)\n",
      "\n",
      "ğŸ” Worst Parameters for KMeans: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 14, 'n_init': 20, 'tol': 0.001}\n",
      "ğŸ“‰ Worst Silhouette Score (Validation): 0.2655\n",
      "\n",
      "\n",
      "ğŸ” Testing on unseen data with the best parameters\n",
      "âœ… Silhouette Score (Testing): 0.2917\n",
      "âœ… Davies-Bouldin Index  (Testing):  0.9754\n",
      "âœ… Calinski-Harabasz Score  (Testing):  21179.5098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert dataset to NumPy array and split into training/validation sets\n",
    "print(\"Step 1: Splitting the dataset \\n\")\n",
    "Y_np = Y.to_numpy()\n",
    "\n",
    "Y_train, Y_test = train_test_split(Y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {Y_train.shape}\")\n",
    "print(f\"Testing set shape: {Y_test.shape}\\n\")\n",
    "\n",
    "# Step 2: Define parameter grid for KMeans\n",
    "print(\"Step 2: Defining the parameter grid for KMeans...\\n\")\n",
    "param_grid_kmeans = {\n",
    "    'n_clusters': [11, 14],\n",
    "    'init': ['k-means++', 'random'],\n",
    "    'n_init': [10, 20],\n",
    "    'max_iter': [300, 500],\n",
    "    'tol': [1e-3, 1e-4],\n",
    "}\n",
    "\n",
    "parameter_combinations = list(ParameterGrid(param_grid_kmeans))\n",
    "total_iterations = len(parameter_combinations)\n",
    "print(f\"Total parameter combinations to evaluate: {total_iterations}\\n\")\n",
    "\n",
    "# Step 3: Initialize tracking for the best and worst parameters\n",
    "print(\"Step 3: Setting up tracking for the best and worst parameters...\\n\")\n",
    "best_score = -1\n",
    "worst_score = np.inf\n",
    "best_params = None\n",
    "worst_params = None\n",
    "\n",
    "# Step 4: Set up K-Fold cross-validation\n",
    "print(\"Step 4: Setting up 3-Fold Cross-Validation...\\n\")\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 5: Start the grid search\n",
    "print(\"Step 5: Starting the grid search...\\n\")\n",
    "current_iteration = 0\n",
    "\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"ğŸŒ€ Iteration {current_iteration}/{total_iterations}: Testing parameters {params}...\\n\")\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    calinski_harabasz_scores = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(Y_train), start=1):\n",
    "        print(f\"  â¡ï¸ Fold {fold}: Splitting data...\")\n",
    "        Y_train_cv, Y_val_cv = Y_train[train_index], Y_train[val_index]\n",
    "\n",
    "        try:\n",
    "            # Initialize and fit KMeans\n",
    "            print(f\"    ğŸ”„ Running KMeans with parameters: {params}...\")\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=params['n_clusters'],\n",
    "                init=params['init'],\n",
    "                n_init=params['n_init'],\n",
    "                max_iter=params['max_iter'],\n",
    "                tol=params['tol'],\n",
    "                random_state=42\n",
    "            )\n",
    "            kmeans.fit(Y_train_cv)\n",
    "\n",
    "            # Predict on the validation fold\n",
    "            labels_val = kmeans.predict(Y_val_cv)\n",
    "\n",
    "            # Compute metrics\n",
    "            silhouette_val = silhouette_score(Y_val_cv, labels_val)\n",
    "            dbi_val = davies_bouldin_score(Y_val_cv, labels_val)\n",
    "            ch_score_val = calinski_harabasz_score(Y_val_cv, labels_val)\n",
    "\n",
    "            # Append scores\n",
    "            silhouette_scores.append(silhouette_val)\n",
    "            davies_bouldin_scores.append(dbi_val)\n",
    "            calinski_harabasz_scores.append(ch_score_val)\n",
    "\n",
    "            print(f\"    âœ… Silhouette Score for Fold {fold}: {silhouette_val:.4f}\")\n",
    "            print(f\"    âœ… Davies-Bouldin Index for Fold {fold}: {dbi_val:.4f}\")\n",
    "            print(f\"    âœ… Calinski-Harabasz Score for Fold {fold}: {ch_score_val:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Error during Fold {fold}: {e}\")\n",
    "            silhouette_scores.append(np.nan)\n",
    "            davies_bouldin_scores.append(np.nan)\n",
    "            calinski_harabasz_scores.append(np.nan)\n",
    "\n",
    "    # Compute average scores for this parameter set\n",
    "    mean_silhouette = np.nanmean(silhouette_scores)\n",
    "    mean_dbi = np.nanmean(davies_bouldin_scores)\n",
    "    mean_ch_score = np.nanmean(calinski_harabasz_scores)\n",
    "\n",
    "    print(f\"  ğŸ“Š Average Silhouette Score: {mean_silhouette:.4f}\")\n",
    "    print(f\"  ğŸ“Š Average Davies-Bouldin Index: {mean_dbi:.4f} (lower is better)\")\n",
    "    print(f\"  ğŸ“Š Average Calinski-Harabasz Score: {mean_ch_score:.4f} (higher is better)\\n\")\n",
    "\n",
    "    # Update the best parameters if a higher Silhouette Score is found\n",
    "    if mean_silhouette > best_score:\n",
    "        print(\"  ğŸ‰ New best parameters found!\")\n",
    "        best_score = mean_silhouette\n",
    "        best_params = params\n",
    "\n",
    "    # Update the worst parameters if a lower Silhouette Score is found\n",
    "    if mean_silhouette < worst_score:\n",
    "        worst_score = mean_silhouette\n",
    "        worst_params = params\n",
    "\n",
    "    print(\"-\" * 50) \n",
    "\n",
    "# Step 6: Print the summary of results\n",
    "print(\"\\nğŸ‰ Grid Search Completed!\")\n",
    "print(f\"ğŸ” Best Parameters for KMeans: {best_params}\")\n",
    "print(f\"ğŸ“ˆ Best Silhouette Score (Validation): {best_score:.4f}\")\n",
    "print(f\"ğŸ“Š Best Davies-Bouldin Index (Validation): {mean_dbi:.4f} (lower is better)\")\n",
    "print(f\"ğŸ“Š Best Calinski-Harabasz Score (Validation): {mean_ch_score:.4f} (higher is better)\\n\")\n",
    "print(f\"ğŸ” Worst Parameters for KMeans: {worst_params}\")\n",
    "print(f\"ğŸ“‰ Worst Silhouette Score (Validation): {worst_score:.4f}\\n\")\n",
    "\n",
    "# Step 7:Testing\n",
    "print(\"\\n Step 7: Evaluating on test set...\")\n",
    "final_kmeans = KMeans(\n",
    "    n_clusters=best_params['n_clusters'],\n",
    "    init=best_params['init'],\n",
    "    n_init=best_params['n_init'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    tol=best_params['tol'],\n",
    "    random_state=42\n",
    ")\n",
    "final_kmeans.fit(Y_train)\n",
    "\n",
    "# Predict on test data\n",
    "labels_test = final_kmeans.predict(Y_test)\n",
    "\n",
    "# Evaluate metrics on test data\n",
    "silhouette_test = silhouette_score(Y_test, labels_test)\n",
    "dbi_test = davies_bouldin_score(Y_test, labels_test)\n",
    "ch_score_test = calinski_harabasz_score(Y_test, labels_test)\n",
    "\n",
    "print(f\"âœ… Silhouette Score (Testing): {silhouette_test:.4f}\")\n",
    "print(f\"âœ… Davies-Bouldin Index  (Testing):  {dbi_test:.4f}\")\n",
    "print(f\"âœ… Calinski-Harabasz Score  (Testing):  {ch_score_test:.4f}\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8bd01",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "#### Best Parameters:\n",
    "- **Initialization (`init`)**: `'k-means++'` produced superior cluster centers compared to `'random'`.\n",
    "- **Clusters (`n_clusters`)**: 11 clusters achieved the best balance and clustering quality.\n",
    "- **Initializations (`n_init`)**: 10 ensured consistent and robust solutions.\n",
    "- **Iterations (`max_iter`)**: 300 were sufficient for convergence.\n",
    "- **Tolerance (`tol`)**: 0.0001 provided stable convergence.\n",
    "- **Best Silhouette Score**: 0.2894 (moderately well-separated clusters).\n",
    "\n",
    "#### Key Findings:\n",
    "- `'k-means++'` initialization consistently outperformed `'random'`.\n",
    "- 11 clusters balanced separation and coherence; 14 clusters led to poor structure.\n",
    "- Increasing `n_init` to 10 improved robustness and avoided poor local minima.\n",
    "- 300 iterations and a tolerance of 0.0001 were sufficient for stable convergence.\n",
    "\n",
    "#### Conclusion:\n",
    "- The best configuration used `'k-means++'` with 11 clusters, 10 initializations, 300 iterations, and a tolerance of 0.0001.\n",
    "- These settings achieved well-defined, meaningful clusters, while poor initialization and excessive clusters degraded performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9b921-4694-4813-b703-26253a132162",
   "metadata": {},
   "source": [
    "### 5.3 t-SNE Pipeline Hyperparameter tuning \n",
    "\n",
    "- TSNE: t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Pupose : This is a technique for reducing the dimensionality of data to two dimensions (n_components=2) for the purpose of visualization.\n",
    "- Metric : KL Diveregnce\n",
    "- Grid search for the optimal t-SNE hyperparameters is performed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eaf6f",
   "metadata": {},
   "source": [
    "**t-SNE is used for `genre_data_cleaned` for below reasons:**\n",
    "- t-SNE is great for visualizing data by reducing it to 2 or 3 dimensions.\n",
    "- It's designed to capture complex, non-linear relationships, making it ideal for exploring clusters, like genres.\n",
    "- However, t-SNE is computationally heavy, so it's best suited for small to medium datasets.\n",
    "\n",
    "**Conclusion:**\n",
    "- Use t-SNE for the `genre_data_cleaned` dataset to visualize clusters and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811f6e7-e703-4936-9355-6b32c8a8bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Splitting the dataset \n",
      "\n",
      "Training set shape: (2378, 13)\n",
      "Test set shape: (595, 13)\n",
      "\n",
      "Step 2: Defining the parameter grid for t-SNE...\n",
      "\n",
      "Total parameter combinations to evaluate: 81\n",
      "\n",
      "Step 3: Setting up tracking for the best parameters...\n",
      "\n",
      "Step 4: Setting up 3-Fold Cross-Validation...\n",
      "\n",
      "Step 5: Starting the grid search...\n",
      "\n",
      "ğŸŒ€ Iteration 1/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8420\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8411\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8516\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8449\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 2/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7451\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7468\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7702\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7540\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 3/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6588\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6634\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6838\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6687\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 4/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7766\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7779\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7881\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7808\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 5/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7289\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7299\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7521\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7370\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 6/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6513\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6567\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6770\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6617\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 7/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7559\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7582\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7680\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7607\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 8/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7242\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7250\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7475\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7323\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 9/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6495\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6550\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6757\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6601\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 10/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8231\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8229\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8353\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8271\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 11/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7383\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7385\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7624\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7464\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 12/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6549\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6598\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6803\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6650\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 13/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7689\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7697\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7814\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7733\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 14/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7271\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7276\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7514\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7354\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 15/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6504\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6558\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6765\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6609\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 16/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7488\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7518\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7620\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7542\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 17/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7222\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7236\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7477\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7312\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 18/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6491\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6548\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6754\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6597\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 19/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8199\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8210\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8314\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8241\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 20/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7364\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7359\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7604\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7442\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 21/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6531\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6587\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6791\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6637\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 22/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7658\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7672\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7773\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7701\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 23/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7253\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7268\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7507\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7343\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 24/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6487\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6557\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6761\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6602\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 25/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7465\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7492\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7594\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7517\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 26/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7214\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7229\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7473\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7305\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 27/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6473\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6547\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6752\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6591\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 28/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8422\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8391\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8563\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8458\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 29/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7450\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7464\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7703\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7539\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 30/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6598\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6635\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6844\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6692\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 31/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7782\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7761\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7942\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7828\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 32/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7292\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7295\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7524\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7370\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 33/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6505\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6565\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6775\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6615\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 34/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7575\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7548\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7719\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7614\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 35/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7240\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7245\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7469\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7318\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 36/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6483\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6553\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6760\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6599\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 37/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8252\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8249\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8394\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8298\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 38/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7382\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7386\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7632\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7466\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 39/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6559\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6603\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6811\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6658\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 40/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7719\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7702\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7846\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7756\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 41/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7273\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7275\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7501\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7350\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 42/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6495\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6561\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6766\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6607\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 43/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7521\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7501\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7664\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7562\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 44/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7226\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7237\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7459\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7307\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 45/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6477\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6552\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6758\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6596\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 46/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8237\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8188\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8326\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8250\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 47/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7357\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7356\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7632\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7449\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 48/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6554\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6591\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6802\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6649\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 49/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7668\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7639\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7784\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7697\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 50/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7254\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7257\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7526\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7346\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 51/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6501\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6558\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6763\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6608\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 52/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7447\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7456\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7587\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7496\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 53/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7215\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7225\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7484\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7308\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 54/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6486\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6553\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6755\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6598\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 55/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8450\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8407\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8538\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8465\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 56/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7452\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7466\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7743\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7554\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 57/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6595\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6643\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6853\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6697\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 58/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7770\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7770\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7900\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7813\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 59/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7296\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7298\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7535\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7376\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 60/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6503\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6567\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6778\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6616\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 61/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7544\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7572\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7701\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7606\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 62/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7242\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7247\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7481\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7323\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 63/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6483\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6553\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6764\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6600\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 64/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8285\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8298\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8438\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8341\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 65/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7389\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7392\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7669\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7483\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 66/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6546\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6606\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6811\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6654\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 67/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7687\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7722\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7808\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7739\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 68/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7264\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7271\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7556\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7364\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 69/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6493\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6558\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6759\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6603\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 70/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7483\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7519\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7618\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7540\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 71/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7219\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7230\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7508\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7319\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 72/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6476\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6545\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6747\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6589\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 73/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.8340\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.8349\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.9047\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.8579\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 74/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7459\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7464\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7775\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7566\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 75/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6624\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6769\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6919\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6771\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 76/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7691\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7696\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.8257\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7881\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 77/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7345\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7342\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7633\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7440\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 78/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6556\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6719\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6872\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6716\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 79/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7485\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7483\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7981\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7649\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 80/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 1: 0.7305\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 2: 0.7302\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "    âœ… KL Divergence for Fold 3: 0.7582\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.7397\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Iteration 81/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "\n",
      "  â¡ï¸ Fold 1: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 1: 0.6536\n",
      "\n",
      "  â¡ï¸ Fold 2: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 2: 0.6707\n",
      "\n",
      "  â¡ï¸ Fold 3: Preparing training and validation data...\n",
      "    ğŸ”„ Running t-SNE with parameters: {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "    âœ… KL Divergence for Fold 3: 0.6861\n",
      "\n",
      "  ğŸ“Š Average KL Divergence for parameters: 0.6701\n",
      "\n",
      "  âš ï¸ Parameters did not improve the best score.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ‰ Grid Search Completed!\n",
      "ğŸ” Best Parameters for t-SNE: {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}\n",
      "ğŸ“‰ Lowest KL Divergence: 0.6589\n",
      "\n",
      "\n",
      "ğŸ” Testing on unseen data with the best parameters\n",
      "âœ… KL Divergence on the test set: 0.4223\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset to NumPy array (X is a DataFrame)\n",
    "X_np = X.to_numpy()\n",
    "\n",
    "# Step 1: Split dataset into training and validation sets\n",
    "print(\"Step 1: Splitting the dataset \\n\")\n",
    "X_train, X_test = train_test_split(X_np, test_size=0.2, random_state=42)\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\\n\")\n",
    "\n",
    "# Step 2: Define parameter grid for t-SNE\n",
    "print(\"Step 2: Defining the parameter grid for t-SNE...\\n\")\n",
    "param_grid_tsne = {\n",
    "    'perplexity': [10, 30, 50],\n",
    "    'learning_rate': [100, 200, 300],\n",
    "    'max_iter': [500, 1000, 2000],\n",
    "    'early_exaggeration': [6, 12, 24],\n",
    "}\n",
    "parameter_combinations = list(ParameterGrid(param_grid_tsne))\n",
    "total_iterations = len(parameter_combinations)\n",
    "print(f\"Total parameter combinations to evaluate: {total_iterations}\\n\")\n",
    "\n",
    "# Step 3: Initialize variables to track the best parameters\n",
    "print(\"Step 3: Setting up tracking for the best parameters...\\n\")\n",
    "best_kl_divergence = np.inf\n",
    "best_tsne_params = None\n",
    "\n",
    "# Step 4: Set up K-Fold cross-validation\n",
    "print(\"Step 4: Setting up 3-Fold Cross-Validation...\\n\")\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 5: Start the grid search\n",
    "print(\"Step 5: Starting the grid search...\\n\")\n",
    "current_iteration = 0\n",
    "\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"ğŸŒ€ Iteration {current_iteration}/{total_iterations}: Testing parameters {params}...\\n\")\n",
    "    kl_divergences = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):\n",
    "        print(f\"  â¡ï¸ Fold {fold}: Preparing training and validation data...\")\n",
    "        X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "\n",
    "        try:\n",
    "            print(f\"    ğŸ”„ Running t-SNE with parameters: {params}...\")\n",
    "            # Initialize and fit t-SNE\n",
    "            tsne = TSNE(\n",
    "                n_components=2,\n",
    "                perplexity=params['perplexity'],\n",
    "                learning_rate=params['learning_rate'],\n",
    "                max_iter=params['max_iter'],\n",
    "                early_exaggeration=params['early_exaggeration'],\n",
    "                random_state=42,\n",
    "                verbose=0\n",
    "            )\n",
    "            tsne.fit(X_train_cv)\n",
    "\n",
    "            # Retrieve KL divergence\n",
    "            kl_divergence = tsne.kl_divergence_\n",
    "            kl_divergences.append(kl_divergence)\n",
    "            print(f\"    âœ… KL Divergence for Fold {fold}: {kl_divergence:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Error during Fold {fold}: {e}\\n\")\n",
    "            kl_divergences.append(np.nan)\n",
    "\n",
    "    # Compute the mean KL divergence across folds\n",
    "    mean_kl_divergence = np.nanmean(kl_divergences)\n",
    "    print(f\"  ğŸ“Š Average KL Divergence for parameters: {mean_kl_divergence:.4f}\\n\")\n",
    "\n",
    "    # Update best parameters if a lower KL divergence is found\n",
    "    if mean_kl_divergence < best_kl_divergence:\n",
    "        print(\"  ğŸ‰ New best parameters found!\")\n",
    "        best_kl_divergence = mean_kl_divergence\n",
    "        best_tsne_params = params\n",
    "    else:\n",
    "        print(\"  âš ï¸ Parameters did not improve the best score.\\n\")\n",
    "\n",
    "    print(\"-\" * 50) \n",
    "\n",
    "# Step 6: Results\n",
    "print(\"\\nğŸ‰ Grid Search Completed!\")\n",
    "print(f\"ğŸ” Best Parameters for t-SNE: {best_tsne_params}\")\n",
    "print(f\"ğŸ“‰ Lowest KL Divergence: {best_kl_divergence:.4f}\\n\")\n",
    "\n",
    "# Step 7:Testing\n",
    "print(\"\\n Step 7: Evaluating on test set...\")\n",
    "tsne_test = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=best_tsne_params['perplexity'],\n",
    "    learning_rate=best_tsne_params['learning_rate'],\n",
    "    max_iter=best_tsne_params['max_iter'],\n",
    "    early_exaggeration=best_tsne_params['early_exaggeration'],\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "tsne_test.fit(X_test)\n",
    "\n",
    "# KL Divergence for the test set\n",
    "test_kl_divergence = tsne_test.kl_divergence_\n",
    "print(f\"âœ… KL Divergence on the test set: {test_kl_divergence:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd4aa5",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "**Best Parameters for t-SNE:**\n",
    "- **KL Divergence (0.7602)** :The lowest value achieved, indicating an effective mapping of high-dimensional data into lower dimensions with minimal loss of structural information.\n",
    "- **Perplexity (50)**:This value balances the local and global aspects of the data, suggesting that the chosen perplexity effectively captures the underlying data structure.\n",
    "- **Learning Rate (300)**:This learning rate allows for stable convergence while avoiding oscillations or slow updates.\n",
    "- **Early Exaggeration(6)**:Enhances separation during the initial iterations, improving visualization of clusters in the final embedding.\n",
    "- **Max Iterations (2000)**:The model converges effectively, reflecting that the chosen number of iterations provides sufficient time for optimization.\n",
    "\n",
    "#### Summary\n",
    "Use this configuration for final t-SNE visualization to ensure optimal preservation of data structure and cluster separability in the low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81ccdd-2ee7-4aaa-8a49-9545db37053a",
   "metadata": {},
   "source": [
    "### 5.4 PCA Pipeline Hyperparameter tuning \n",
    "\n",
    "- PCA: Principal Component Analysis (PCA)\n",
    "- Purpose: A technique for dimensionality reduction that transforms the data into a lower-dimensional space while retaining as much variance as possible.\n",
    "- Metric: Reconstrction error (Mean squared error)\n",
    "- Grid search for the optimal PCA is performed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a58b46",
   "metadata": {},
   "source": [
    "**We use PCA for `data_cleaned` for below reasons:**\n",
    "- PCA is efficient for reducing dimensions while keeping most of the original data's variance.\n",
    "- It's better for preprocessing large datasets because it is faster and scales well.\n",
    "- PCA assumes linear relationships, which works well for structured data intended for modeling.\n",
    "\n",
    "**Conclusion:**\n",
    "- Use PCA for the `data_cleaned` dataset to prepare it for further analysis or machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c86e5-9400-44dd-a560-c86e39c9e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Splitting the dataset into training,validation and test sets...\n",
      "\n",
      "Training set shape: (136522, 17)\n",
      "Test set shape: (34131, 17)\n",
      "\n",
      "Step 2: Defining the parameter grid for PCA...\n",
      "\n",
      "Total parameter combinations to evaluate: 12\n",
      "\n",
      "Step 3: Setting up tracking for the best parameters...\n",
      "\n",
      "Step 4: Setting up 3-Fold Cross-Validation...\n",
      "\n",
      "Step 5: Starting the grid search...\n",
      "\n",
      "ğŸŒ€ Evaluating combination 1/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "  ğŸ‰ New best parameters found!\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 2/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': False}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 3/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': True}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 4/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': False}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 5/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 6/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': False}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 7/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': True}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 8/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': False}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3938\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4482\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 9/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3937\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4483\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 10/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': False}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3937\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4483\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 11/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': True}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3937\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': True}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4483\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "ğŸŒ€ Evaluating combination 12/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': False}\n",
      "  â¡ï¸ Fold 1: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 1: 459668.3937\n",
      "\n",
      "  â¡ï¸ Fold 2: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 2: 459714.6201\n",
      "\n",
      "  â¡ï¸ Fold 3: Splitting data...\n",
      "    ğŸ”„ Running PCA with parameters: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': False}...\n",
      "    âœ… Reconstruction Error for Fold 3: 459713.4483\n",
      "\n",
      "  ğŸ“Š Average Reconstruction Error for parameters: 459698.8207\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Step 6: Evaluating the best parameters on the test set...\n",
      "\n",
      "\n",
      "ğŸ‰ Grid Search Completed!\n",
      "ğŸ” Best Parameters for PCA: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "ğŸ“‰ Lowest Average Reconstruction Error (Validation): 459698.8207\n",
      "ğŸ“ˆ Reconstruction Error on Test Set: 459695.2128\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset to NumPy array (Y is a DataFrame)\n",
    "Y_np = Y.to_numpy()\n",
    "\n",
    "# Step 1: Split dataset into training and test sets\n",
    "print(\"Step 1: Splitting the dataset \\n\")\n",
    "Y_train, Y_test = train_test_split(Y_np, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {Y_train.shape}\")\n",
    "print(f\"Test set shape: {Y_test.shape}\\n\")\n",
    "\n",
    "# Step 2: Define parameter grid for PCA\n",
    "print(\"Step 2: Defining the parameter grid for PCA...\\n\")\n",
    "param_grid_pca = {\n",
    "    'PCA__n_components': [2],                       # Range of components\n",
    "    'PCA__whiten': [True, False],                   # Whitening options\n",
    "    'PCA__svd_solver': ['auto', 'full', 'randomized'],  # Solver options\n",
    "    'PCA__tol': [1e-4, 1e-3],                      # Tolerance for convergence\n",
    "}\n",
    "\n",
    "parameter_combinations = list(ParameterGrid(param_grid_pca))\n",
    "total_combinations = len(parameter_combinations)\n",
    "print(f\"Total parameter combinations to evaluate: {total_combinations}\\n\")\n",
    "\n",
    "# Step 3: Initialize variables for tracking the best parameters\n",
    "print(\"Step 3: Setting up tracking for the best parameters...\\n\")\n",
    "best_error = np.inf\n",
    "best_params = None\n",
    "current_iteration = 0\n",
    "\n",
    "# Step 4: Set up K-Fold Cross-Validation\n",
    "print(\"Step 4: Setting up 3-Fold Cross-Validation...\\n\")\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Step 5: Start the grid search\n",
    "print(\"Step 5: Starting the grid search...\\n\")\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"ğŸŒ€ Evaluating combination {current_iteration}/{total_combinations}: {params}\")\n",
    "\n",
    "    fold_errors = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(Y_train), start=1):\n",
    "        print(f\"  â¡ï¸ Fold {fold}: Splitting data...\")\n",
    "        Y_train_cv, Y_val_cv = Y_train[train_index], Y_train[val_index]\n",
    "\n",
    "        try:\n",
    "            # Define the PCA pipeline with current parameters\n",
    "            print(f\"    ğŸ”„ Running PCA with parameters: {params}...\")\n",
    "            pca_pipeline = Pipeline([\n",
    "                ('scaler', StandardScaler()),  # Standardize the features\n",
    "                ('PCA', PCA(\n",
    "                    n_components=params['PCA__n_components'],\n",
    "                    whiten=params['PCA__whiten'],\n",
    "                    svd_solver=params['PCA__svd_solver'],\n",
    "                    tol=params.get('PCA__tol', None),\n",
    "                    random_state=42\n",
    "                ))\n",
    "            ])\n",
    "\n",
    "            # Fit the pipeline on training fold and transform validation fold\n",
    "            pca_pipeline.fit(Y_train_cv)\n",
    "            transformed_val = pca_pipeline.transform(Y_val_cv)\n",
    "\n",
    "            # Inverse transform the validation data for reconstruction\n",
    "            reconstructed_val = pca_pipeline.named_steps['PCA'].inverse_transform(transformed_val)\n",
    "\n",
    "            # Calculate reconstruction error on validation data\n",
    "            error = mean_squared_error(Y_val_cv, reconstructed_val)\n",
    "            fold_errors.append(error)\n",
    "            print(f\"    âœ… Reconstruction Error for Fold {fold}: {error:.4f}\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    âŒ Error during Fold {fold}: {e}\\n\")\n",
    "            fold_errors.append(np.nan)\n",
    "\n",
    "    # Compute the mean reconstruction error across folds\n",
    "    mean_error = np.nanmean(fold_errors)\n",
    "    print(f\"  ğŸ“Š Average Reconstruction Error for parameters: {mean_error:.4f}\\n\")\n",
    "\n",
    "    # Update the best parameters if the average reconstruction error improves\n",
    "    if mean_error < best_error:\n",
    "        print(\"  ğŸ‰ New best parameters found!\")\n",
    "        best_error = mean_error\n",
    "        best_params = params\n",
    "\n",
    "    print(\"-\" * 50)  \n",
    "\n",
    "# Step 6: Evaluate the best parameters on the test set\n",
    "print(\"\\nStep 6: Evaluating on test set...\\n\")\n",
    "final_pca_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize the features\n",
    "    ('PCA', PCA(\n",
    "        n_components=best_params['PCA__n_components'],\n",
    "        whiten=best_params['PCA__whiten'],\n",
    "        svd_solver=best_params['PCA__svd_solver'],\n",
    "        tol=best_params.get('PCA__tol', None),\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "final_pca_pipeline.fit(Y_train)\n",
    "transformed_test = final_pca_pipeline.transform(Y_test)\n",
    "reconstructed_test = final_pca_pipeline.named_steps['PCA'].inverse_transform(transformed_test)\n",
    "test_error = mean_squared_error(Y_test, reconstructed_test)\n",
    "\n",
    "# Final Output\n",
    "print(\"\\nğŸ‰ Grid Search Completed!\")\n",
    "print(f\"ğŸ” Best Parameters for PCA: {best_params}\")\n",
    "print(f\"ğŸ“‰ Lowest Average Reconstruction Error (Validation): {best_error:.4f}\")\n",
    "print(f\"ğŸ“ˆ Reconstruction Error on Test Set: {test_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29fdec",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "- **Reconstruction Error (459695.21)** PCA successfully reduced dimensionality without significant data loss.\n",
    "- **Tolerance(0.0001)** Ensures PCA converges with high precision by preventing premature stopping.\n",
    "- **PCA_svd_solver('auto')** The best solver is automatically choosen based on dataset size, balancing speed and accuracy.\n",
    "\n",
    "#### Summary\n",
    "The best parameters provide a good balance between efficiency and minimizing reconstruction error by ensuring optimal preservation of data structure and maintaining meaningful cluster separability in the reduced-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5bdd5-a42c-43fb-8f15-606544bf67f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
