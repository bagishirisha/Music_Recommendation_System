{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66fd37e-6415-4ef5-88a4-f8f4ede27df0",
   "metadata": {},
   "source": [
    "## Step 05: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b776fba-f31d-4e5f-aa6e-a6e5094117d0",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b88b99d-ceca-412c-b434-0265d24699b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score,calinski_harabasz_score\n",
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0eddca-4c08-4341-ae02-b2a8b89fa23e",
   "metadata": {},
   "source": [
    "### 5.1 Load datasets from file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6723b340-a460-4e99-9dec-e6a4f563842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: Data and genre data successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "data_path = '../Dataset/data_cleaned.csv'\n",
    "genre_data_path = '../Dataset/genre_data_cleaned.csv'\n",
    "\n",
    "# Check if files exist and load them\n",
    "if os.path.exists(data_path) and os.path.exists(genre_data_path):\n",
    "    data = pd.read_csv(data_path)\n",
    "    genre_data = pd.read_csv(genre_data_path)\n",
    "    print(\"Info: Data and genre data successfully loaded.\")\n",
    "else:\n",
    "    print(\"Attention: One or both files are not found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737cb614-af97-4132-b364-d968c5abbb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = genre_data.select_dtypes(include=np.number) \n",
    "Y = data.select_dtypes(include=np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0059690-9476-4d8c-86b2-a0097cf88e88",
   "metadata": {},
   "source": [
    "### 5.2  KMeans Hyperparameter tuning \n",
    "- Exhaustive grid search is perfomed over KMeans hyperparameters using a training-validation split of 80%-20%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dec85-7774-4a76-a66b-5adcccae249d",
   "metadata": {},
   "source": [
    "#### 5.2.1 Genre_data_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53e9f9b-d8d0-471c-b1e4-017424bcd5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to evaluate: 32\n",
      "\n",
      "Iteration 1/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2878, Davies-Bouldin Score: 0.9162,Calinski Harabasz Score: 316.1541\n",
      "Iteration 2/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2860, Davies-Bouldin Score: 0.9133,Calinski Harabasz Score: 315.4086\n",
      "Iteration 3/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2846, Davies-Bouldin Score: 0.9270,Calinski Harabasz Score: 313.9262\n",
      "Iteration 4/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2846, Davies-Bouldin Score: 0.9270,Calinski Harabasz Score: 313.9262\n",
      "Iteration 5/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 6/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 7/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 8/32: Testing parameters {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 9/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2878, Davies-Bouldin Score: 0.9162,Calinski Harabasz Score: 316.1541\n",
      "Iteration 10/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2860, Davies-Bouldin Score: 0.9133,Calinski Harabasz Score: 315.4086\n",
      "Iteration 11/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2846, Davies-Bouldin Score: 0.9270,Calinski Harabasz Score: 313.9262\n",
      "Iteration 12/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2846, Davies-Bouldin Score: 0.9270,Calinski Harabasz Score: 313.9262\n",
      "Iteration 13/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 14/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 15/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 16/32: Testing parameters {'init': 'k-means++', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2355, Davies-Bouldin Score: 1.0288,Calinski Harabasz Score: 275.6485\n",
      "Iteration 17/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2789, Davies-Bouldin Score: 0.9493,Calinski Harabasz Score: 316.3645\n",
      "Iteration 18/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2785, Davies-Bouldin Score: 0.9494,Calinski Harabasz Score: 315.7175\n",
      "Iteration 19/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2789, Davies-Bouldin Score: 0.9493,Calinski Harabasz Score: 316.3645\n",
      "Iteration 20/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2785, Davies-Bouldin Score: 0.9494,Calinski Harabasz Score: 315.7175\n",
      "Iteration 21/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2326, Davies-Bouldin Score: 1.0363,Calinski Harabasz Score: 271.1233\n",
      "Iteration 22/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2326, Davies-Bouldin Score: 1.0363,Calinski Harabasz Score: 271.1233\n",
      "Iteration 23/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Iteration 24/32: Testing parameters {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Iteration 25/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2789, Davies-Bouldin Score: 0.9493,Calinski Harabasz Score: 316.3645\n",
      "Iteration 26/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2785, Davies-Bouldin Score: 0.9494,Calinski Harabasz Score: 315.7175\n",
      "Iteration 27/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2789, Davies-Bouldin Score: 0.9493,Calinski Harabasz Score: 316.3645\n",
      "Iteration 28/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 13, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2785, Davies-Bouldin Score: 0.9494,Calinski Harabasz Score: 315.7175\n",
      "Iteration 29/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2326, Davies-Bouldin Score: 1.0363,Calinski Harabasz Score: 271.1233\n",
      "Iteration 30/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 10, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2326, Davies-Bouldin Score: 1.0363,Calinski Harabasz Score: 271.1233\n",
      "Iteration 31/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.001}...\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Iteration 32/32: Testing parameters {'init': 'random', 'max_iter': 500, 'n_clusters': 20, 'n_init': 20, 'tol': 0.0001}...\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "\n",
      "Search Completed!\n",
      "Best Parameters for KMeans: {'init': 'k-means++', 'max_iter': 300, 'n_clusters': 13, 'n_init': 10, 'tol': 0.001}\n",
      "Best Silhouette Score (Validation): 0.28778655157681615\n",
      "\n",
      "Worst Parameters for KMeans: {'init': 'random', 'max_iter': 300, 'n_clusters': 20, 'n_init': 10, 'tol': 0.001}\n",
      "Worst Silhouette Score (Validation): 0.2326156568404128\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and validation sets to prevent overfitting\n",
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define an exhaustive parameter grid for KMeans\n",
    "param_grid_kmeans = {\n",
    "    'n_clusters': [13, 20],                       # Reasonable cluster range (based on elbow Method)\n",
    "    'init': ['k-means++', 'random'],              # Initialization methods\n",
    "    'n_init': [10, 20],                           # Number of initializations\n",
    "    'max_iter': [300, 500],                       # Maximum iterations\n",
    "    'tol': [1e-3, 1e-4]                           # Convergence tolerance\n",
    "}\n",
    "\n",
    "# Track progress\n",
    "parameter_combinations = list(ParameterGrid(param_grid_kmeans))\n",
    "total_iterations = len(parameter_combinations)\n",
    "current_iteration = 0\n",
    "\n",
    "# Track the best and worst parameters and scores\n",
    "best_score = -1\n",
    "worst_score = np.inf\n",
    "best_params = None\n",
    "worst_params = None\n",
    "\n",
    "print(f\"Total parameter combinations to evaluate: {total_iterations}\\n\")\n",
    "\n",
    "# Perform grid search with training-validation split\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"Iteration {current_iteration}/{total_iterations}: Testing parameters {params}...\")\n",
    "\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=params['n_clusters'],\n",
    "        init=params['init'],\n",
    "        n_init=params['n_init'],\n",
    "        max_iter=params['max_iter'],\n",
    "        tol=params['tol'],\n",
    "        random_state=42\n",
    "    )\n",
    "    try:\n",
    "        # Fit on training data\n",
    "        kmeans.fit(X_train)\n",
    "        labels_train = kmeans.labels_\n",
    "\n",
    "        # Predict on validation data\n",
    "        labels_val = kmeans.predict(X_val)\n",
    "\n",
    "        # Compute metrics on validation data\n",
    "        silhouette_val = silhouette_score(X_val, labels_val)\n",
    "        davies_bouldin_val = davies_bouldin_score(X_val, labels_val)\n",
    "        calinski_harabasz_val = calinski_harabasz_score(X_val, labels_val)\n",
    "\n",
    "\n",
    "        print(f\"Silhouette Score: {silhouette_val:.4f}, Davies-Bouldin Score: {davies_bouldin_val:.4f},Calinski Harabasz Score: {calinski_harabasz_val:.4f}\")\n",
    "\n",
    "        # Update the best parameters if validation silhouette score improves\n",
    "        if silhouette_val > best_score:\n",
    "            best_score = silhouette_val\n",
    "            best_params = params\n",
    "\n",
    "        # Update the worst parameters if validation silhouette score decreases\n",
    "        if silhouette_val < worst_score:\n",
    "            worst_score = silhouette_val\n",
    "            worst_params = params\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Iteration {current_iteration}/{total_iterations} failed with error: {e}\")\n",
    "\n",
    "# Print the final best and worst parameters and silhouette scores\n",
    "print(\"\\nSearch Completed!\")\n",
    "print(\"Best Parameters for KMeans:\", best_params)\n",
    "print(\"Best Silhouette Score (Validation):\", best_score)\n",
    "print(\"\\nWorst Parameters for KMeans:\", worst_params)\n",
    "print(\"Worst Silhouette Score (Validation):\", worst_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f936e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Conclusion\n",
    "**Best hyperparameter values are**:\n",
    "\n",
    "**Initialization Method**\n",
    "- k-means initialization provided better centroids compared to random initialization.\n",
    "\n",
    "**Cluster Count**\n",
    "- Using 13 clusters results in better-defined groups compared to 20 clusters which lead poor separated clusters.\n",
    "\n",
    "**Tolerance Impact**\n",
    "- Smaller tolerances (0.0001) were not significantly better than 0.001, indicating stability.\n",
    "\n",
    "**Iteration Count**\n",
    "- 300 iterations were sufficient for convergence.\n",
    "\n",
    "**Evaluation Metrics(for best parameters)**\n",
    "- Silhouette Score (0.2878): Indicates moderately defined clusters with relatively high cohesion within clusters.Clusters are not distinct.\n",
    "- Davies-Bouldin Score (0.9162): A lower score compared to other configurations, highlighting good separation and compactness of clusters.\n",
    "- Calinski-Harabasz Score (316.1541): The highest score, suggesting compact and well-separated clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a117868-42e8-4180-bf8d-5d5c8470c8d4",
   "metadata": {},
   "source": [
    "#### 5.2.2 Data_Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a93c282c-3c42-4f7d-bc4b-e3b831eb4ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "Parameter grid defined.\n",
      "Data sampled for tuning.\n",
      "Testing parameter set 1/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 2/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 3/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 4/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 5/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 6/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 7/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 8/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 9/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 10/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 11/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 12/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 13/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 14/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 15/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 16/32: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 17/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 18/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 19/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 20/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 21/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 22/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 23/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 24/32: {'kmeans__init': 'random', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 25/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 26/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 27/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 28/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 11, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 29/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 30/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 31/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Testing parameter set 32/32: {'kmeans__init': 'random', 'kmeans__max_iter': 500, 'kmeans__n_clusters': 14, 'kmeans__n_init': 20, 'kmeans__tol': 0.0001}\n",
      "Model fitting completed.\n",
      "Model prediction completed.\n",
      "Silhouette Score: 0.2420, Davies-Bouldin Score: 0.9869,Calinski Harabasz Score: 280.5569\n",
      "Hyperparameter tuning completed.\n",
      "Best Parameters for KMeans: {'kmeans__init': 'k-means++', 'kmeans__max_iter': 300, 'kmeans__n_clusters': 11, 'kmeans__n_init': 10, 'kmeans__tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing subsets\n",
    "Y_train, Y_test = train_test_split(Y, test_size=0.2, random_state=42)\n",
    "print(\"Data split into training and testing sets.\")\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid_kmeans = {\n",
    "    'kmeans__n_clusters': [11,14],                  # Reasonable (based on Elbow Method)\n",
    "    'kmeans__init': ['k-means++', 'random'],\n",
    "    'kmeans__n_init': [10, 20],\n",
    "    'kmeans__max_iter': [300, 500],\n",
    "    'kmeans__tol': [1e-3, 1e-4]\n",
    "}\n",
    "print(\"Parameter grid defined.\")\n",
    "\n",
    "# Sample a smaller portion of the data to manage memory usage\n",
    "sampled_Y_train = Y_train.sample(frac=0.1, random_state=42)\n",
    "sampled_Y_test = Y_test.sample(frac=0.1, random_state=42)\n",
    "print(\"Data sampled for tuning.\")\n",
    "\n",
    "# Perform hyperparameter tuning on the sampled data\n",
    "results = []\n",
    "for index, params in enumerate(ParameterGrid(param_grid_kmeans)):\n",
    "    print(f\"Testing parameter set {index + 1}/{len(ParameterGrid(param_grid_kmeans))}: {params}\")\n",
    "\n",
    "    # Set up the pipeline with the current parameters\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('kmeans', KMeans(\n",
    "            n_clusters=params['kmeans__n_clusters'],\n",
    "            init=params['kmeans__init'],\n",
    "            n_init=params['kmeans__n_init'],\n",
    "            max_iter=params['kmeans__max_iter'],\n",
    "            tol=params['kmeans__tol'],\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Fit the model on the sampled training data\n",
    "    pipeline.fit(sampled_Y_train)\n",
    "    print(\"Model fitting completed.\")\n",
    "\n",
    "    # Predict on the sampled testing data\n",
    "    labels_test = pipeline.predict(sampled_Y_test)\n",
    "    print(\"Model prediction completed.\")\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    silhouette_test = silhouette_score(sampled_Y_test, labels_test)\n",
    "    davies_bouldin_test = davies_bouldin_score(sampled_Y_test, labels_test)\n",
    "    calinski_harabasz_val = calinski_harabasz_score(X_val, labels_val)\n",
    "\n",
    "\n",
    "    print(f\"Silhouette Score: {silhouette_val:.4f}, Davies-Bouldin Score: {davies_bouldin_val:.4f},Calinski Harabasz Score: {calinski_harabasz_val:.4f}\")\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'silhouette_score': silhouette_test,\n",
    "        'davies_bouldin_score': davies_bouldin_test\n",
    "    })\n",
    "\n",
    "print(\"Hyperparameter tuning completed.\")\n",
    "\n",
    "# Find and print the best parameters\n",
    "results_df = pd.DataFrame(results)\n",
    "best_result = results_df.sort_values(by='silhouette_score', ascending=False).iloc[0]\n",
    "print(\"Best Parameters for KMeans:\", best_result['params'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8bd01",
   "metadata": {},
   "source": [
    "##### Conclusion\n",
    "\n",
    "**Best hyperparameter values are**:\n",
    "\n",
    "**Initialization Method**\n",
    "- k-means initialization provided better centroids compared to random initialization.\n",
    "\n",
    "**Tolerance Impact**\n",
    "- Smaller tolerances (0.0001) were not significantly better than 0.001, indicating stability.\n",
    "\n",
    "**Iteration Count**\n",
    "- 300 iterations were sufficient for convergence.\n",
    "\n",
    "**Clusters**\n",
    "- Using 11 clusters results in better-defined groups compared to 14 clusters which lead to overlapping.\n",
    "\n",
    "**Evaluation Metrics**\n",
    "- Silhouette Score (0.2420): Indicates moderate cluster separation with overlapping boundaries. Clusters are not distinct.\n",
    "- Davies-Bouldin Score (0.9869): Moderately low, suggesting average separation and compactness between clusters.\n",
    "- Calinski-Harabasz Score (280.5569): A moderate value indicating clusters compact but not well-separated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb9b921-4694-4813-b703-26253a132162",
   "metadata": {},
   "source": [
    "### 5.3 t-SNE Pipeline Hyperparameter tuning \n",
    "\n",
    "- TSNE: t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "- Pupose : This is a technique for reducing the dimensionality of data to two dimensions (n_components=2) for the purpose of visualization.\n",
    "- Metric : KL Diveregnce\n",
    "- Grid search for the optimal t-SNE hyperparameters is performed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72eaf6f",
   "metadata": {},
   "source": [
    "**t-SNE is used for `genre_data_cleaned` for below reasons:**\n",
    "- t-SNE is great for visualizing data by reducing it to 2 or 3 dimensions.\n",
    "- It's designed to capture complex, non-linear relationships, making it ideal for exploring clusters, like genres.\n",
    "- However, t-SNE is computationally heavy, so it's best suited for small to medium datasets.\n",
    "\n",
    "**Conclusion:**\n",
    "- Use t-SNE for the `genre_data_cleaned` dataset to visualize clusters and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4811f6e7-e703-4936-9355-6b32c8a8bf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to evaluate: 81\n",
      "\n",
      "Iteration 1/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9862\n",
      "Iteration 2/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8759\n",
      "Iteration 3/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7866\n",
      "Iteration 4/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8664\n",
      "Iteration 5/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8373\n",
      "Iteration 6/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7658\n",
      "Iteration 7/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8326\n",
      "Iteration 8/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8274\n",
      "Iteration 9/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7612\n",
      "Iteration 10/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9469\n",
      "Iteration 11/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8547\n",
      "Iteration 12/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7760\n",
      "Iteration 13/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8543\n",
      "Iteration 14/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8336\n",
      "Iteration 15/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7639\n",
      "Iteration 16/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8254\n",
      "Iteration 17/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8244\n",
      "Iteration 18/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7608\n",
      "Iteration 19/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9383\n",
      "Iteration 20/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8506\n",
      "Iteration 21/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7731\n",
      "Iteration 22/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8492\n",
      "Iteration 23/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8317\n",
      "Iteration 24/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7633\n",
      "Iteration 25/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8211\n",
      "Iteration 26/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8233\n",
      "Iteration 27/81: Testing parameters {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7602\n",
      "Iteration 28/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9951\n",
      "Iteration 29/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8748\n",
      "Iteration 30/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7878\n",
      "Iteration 31/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8726\n",
      "Iteration 32/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8359\n",
      "Iteration 33/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7671\n",
      "Iteration 34/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8397\n",
      "Iteration 35/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8267\n",
      "Iteration 36/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7624\n",
      "Iteration 37/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9546\n",
      "Iteration 38/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8555\n",
      "Iteration 39/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7779\n",
      "Iteration 40/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8621\n",
      "Iteration 41/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8326\n",
      "Iteration 42/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7647\n",
      "Iteration 43/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8322\n",
      "Iteration 44/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8245\n",
      "Iteration 45/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7615\n",
      "Iteration 46/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9463\n",
      "Iteration 47/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8492\n",
      "Iteration 48/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7756\n",
      "Iteration 49/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8579\n",
      "Iteration 50/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8300\n",
      "Iteration 51/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7639\n",
      "Iteration 52/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8280\n",
      "Iteration 53/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8229\n",
      "Iteration 54/81: Testing parameters {'early_exaggeration': 12, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7610\n",
      "Iteration 55/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9932\n",
      "Iteration 56/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8754\n",
      "Iteration 57/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7882\n",
      "Iteration 58/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8723\n",
      "Iteration 59/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8376\n",
      "Iteration 60/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7675\n",
      "Iteration 61/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8425\n",
      "Iteration 62/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8287\n",
      "Iteration 63/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 100, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7626\n",
      "Iteration 64/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9538\n",
      "Iteration 65/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8564\n",
      "Iteration 66/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7780\n",
      "Iteration 67/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8610\n",
      "Iteration 68/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8336\n",
      "Iteration 69/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7656\n",
      "Iteration 70/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8341\n",
      "Iteration 71/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8265\n",
      "Iteration 72/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 200, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7618\n",
      "Iteration 73/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 10}...\n",
      "KL Divergence: 0.9509\n",
      "Iteration 74/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 30}...\n",
      "KL Divergence: 0.8509\n",
      "Iteration 75/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 500, 'perplexity': 50}...\n",
      "KL Divergence: 0.7756\n",
      "Iteration 76/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8573\n",
      "Iteration 77/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8323\n",
      "Iteration 78/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 1000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7654\n",
      "Iteration 79/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 10}...\n",
      "KL Divergence: 0.8298\n",
      "Iteration 80/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 30}...\n",
      "KL Divergence: 0.8260\n",
      "Iteration 81/81: Testing parameters {'early_exaggeration': 24, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}...\n",
      "KL Divergence: 0.7621\n",
      "\n",
      "Search Completed!\n",
      "Best Parameters for t-SNE: {'early_exaggeration': 6, 'learning_rate': 300, 'max_iter': 2000, 'perplexity': 50}\n",
      "Lowest KL Divergence: 0.7602002024650574\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into subsamples for validation to prevent overfitting\n",
    "X_train, X_val = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define an extended parameter grid for t-SNE\n",
    "param_grid_tsne = {\n",
    "    'perplexity': [10, 30, 50],                             # Cover a wider range of perplexities\n",
    "    'learning_rate': [100, 200, 300],                       # Extend learning rate values\n",
    "    'max_iter': [500, 1000, 2000],                          # Include shorter iterations for quick results\n",
    "    'early_exaggeration': [6, 12, 24],                      # Test more exaggeration values\n",
    "}\n",
    "\n",
    "# Prepare for tracking progress\n",
    "parameter_combinations = list(ParameterGrid(param_grid_tsne))\n",
    "total_iterations = len(parameter_combinations)\n",
    "current_iteration = 0\n",
    "\n",
    "# Initialize tracking for the best parameters\n",
    "best_kl_divergence = np.inf\n",
    "best_tsne_params = None\n",
    "\n",
    "# Evaluate t-SNE configurations with progress updates\n",
    "print(f\"Total parameter combinations to evaluate: {total_iterations}\\n\")\n",
    "\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"Iteration {current_iteration}/{total_iterations}: Testing parameters {params}...\")\n",
    "    \n",
    "    try:\n",
    "        # Fit t-SNE on training data\n",
    "        tsne = TSNE(\n",
    "            n_components=2,\n",
    "            perplexity=params['perplexity'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            max_iter=params['max_iter'],\n",
    "            early_exaggeration=params['early_exaggeration'],\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        embedding_train = tsne.fit_transform(X_train)\n",
    "\n",
    "        # Compute KL divergence as the primary metric\n",
    "        kl_divergence = tsne.kl_divergence_\n",
    "\n",
    "        print(f\"KL Divergence: {kl_divergence:.4f}\")\n",
    "\n",
    "        # Update best parameters based on KL divergence\n",
    "        if kl_divergence < best_kl_divergence:\n",
    "            best_kl_divergence = kl_divergence\n",
    "            best_tsne_params = params\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Iteration {current_iteration}/{total_iterations} failed with error: {e}\")\n",
    "\n",
    "# Print the best parameters and lowest KL divergence\n",
    "print(\"\\nSearch Completed!\")\n",
    "print(\"Best Parameters for t-SNE:\", best_tsne_params)\n",
    "print(\"Lowest KL Divergence:\", best_kl_divergence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dd4aa5",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "**Best Parameters for t-SNE:**\n",
    "- **KL Divergence (0.7602)** :The lowest value achieved, indicating an effective mapping of high-dimensional data into lower dimensions with minimal loss of structural information.\n",
    "- **Perplexity (50)**:This value balances the local and global aspects of the data, suggesting that the chosen perplexity effectively captures the underlying data structure.\n",
    "- **Learning Rate (300)**:This learning rate allows for stable convergence while avoiding oscillations or slow updates.\n",
    "- **Early Exaggeration(6)**:Enhances separation during the initial iterations, improving visualization of clusters in the final embedding.\n",
    "- **Max Iterations (2000)**:The model converges effectively, reflecting that the chosen number of iterations provides sufficient time for optimization.\n",
    "\n",
    "#### Summary\n",
    "Use this configuration for final t-SNE visualization to ensure optimal preservation of data structure and cluster separability in the low-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81ccdd-2ee7-4aaa-8a49-9545db37053a",
   "metadata": {},
   "source": [
    "### 5.4 PCA Pipeline Hyperparameter tuning \n",
    "\n",
    "- PCA: Principal Component Analysis (PCA)\n",
    "- Purpose: A technique for dimensionality reduction that transforms the data into a lower-dimensional space while retaining as much variance as possible.\n",
    "- Metric: Reconstrction error (Mean squared error)\n",
    "- Grid search for the optimal PCA is performed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a58b46",
   "metadata": {},
   "source": [
    "**We use PCA for `data_cleaned` for below reasons:**\n",
    "- PCA is efficient for reducing dimensions while keeping most of the original data's variance.\n",
    "- It's better for preprocessing large datasets because it is faster and scales well.\n",
    "- PCA assumes linear relationships, which works well for structured data intended for modeling.\n",
    "\n",
    "**Conclusion:**\n",
    "- Use PCA for the `data_cleaned` dataset to prepare it for further analysis or machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "466c86e5-9400-44dd-a560-c86e39c9e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to evaluate: 12\n",
      "Evaluating combination 1/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "Reconstruction Error (Test) for combination 1/12: 459695.2128\n",
      "✅ New Best Reconstruction Error: 459695.2128 with parameters {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "Evaluating combination 2/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': False}\n",
      "Reconstruction Error (Test) for combination 2/12: 459695.2128\n",
      "Evaluating combination 3/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': True}\n",
      "Reconstruction Error (Test) for combination 3/12: 459695.2128\n",
      "Evaluating combination 4/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.001, 'PCA__whiten': False}\n",
      "Reconstruction Error (Test) for combination 4/12: 459695.2128\n",
      "Evaluating combination 5/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "Reconstruction Error (Test) for combination 5/12: 459695.2128\n",
      "Evaluating combination 6/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.0001, 'PCA__whiten': False}\n",
      "Reconstruction Error (Test) for combination 6/12: 459695.2128\n",
      "Evaluating combination 7/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': True}\n",
      "Reconstruction Error (Test) for combination 7/12: 459695.2128\n",
      "Evaluating combination 8/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'full', 'PCA__tol': 0.001, 'PCA__whiten': False}\n",
      "Reconstruction Error (Test) for combination 8/12: 459695.2128\n",
      "Evaluating combination 9/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "Reconstruction Error (Test) for combination 9/12: 459695.2130\n",
      "Evaluating combination 10/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.0001, 'PCA__whiten': False}\n",
      "Reconstruction Error (Test) for combination 10/12: 459695.2130\n",
      "Evaluating combination 11/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': True}\n",
      "Reconstruction Error (Test) for combination 11/12: 459695.2130\n",
      "Evaluating combination 12/12: {'PCA__n_components': 2, 'PCA__svd_solver': 'randomized', 'PCA__tol': 0.001, 'PCA__whiten': False}\n",
      "Reconstruction Error (Test) for combination 12/12: 459695.2130\n",
      "\n",
      "Search Completed!\n",
      "Best Parameters for PCA: {'PCA__n_components': 2, 'PCA__svd_solver': 'auto', 'PCA__tol': 0.0001, 'PCA__whiten': True}\n",
      "Lowest Reconstruction Error (Test): 459695.2127617188\n"
     ]
    }
   ],
   "source": [
    "Y_train, Y_test = train_test_split(Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for PCA\n",
    "param_grid_pca = {\n",
    "    'PCA__n_components': [2],                  # Range of components\n",
    "    'PCA__whiten': [True, False],              # Whitening options\n",
    "    'PCA__svd_solver': ['auto', 'full', 'randomized'],  # Additional solvers\n",
    "    'PCA__tol': [1e-4, 1e-3]                   # Tolerance for convergence\n",
    "}\n",
    "\n",
    "# Track progress\n",
    "parameter_combinations = list(ParameterGrid(param_grid_pca))\n",
    "total_combinations = len(parameter_combinations)\n",
    "current_iteration = 0\n",
    "\n",
    "# Track the best parameters and lowest reconstruction error\n",
    "best_error = np.inf\n",
    "best_params = None\n",
    "\n",
    "# Perform grid search with train-test split\n",
    "print(f\"Total parameter combinations to evaluate: {total_combinations}\")\n",
    "\n",
    "for params in parameter_combinations:\n",
    "    current_iteration += 1\n",
    "    print(f\"Evaluating combination {current_iteration}/{total_combinations}: {params}\")\n",
    "\n",
    "    try:\n",
    "        # Define the PCA pipeline with the current parameters\n",
    "        pca_pipeline = Pipeline([\n",
    "            ('scaler', StandardScaler()),                               # Standardize the features\n",
    "            ('PCA', PCA(\n",
    "                n_components=params['PCA__n_components'],\n",
    "                whiten=params['PCA__whiten'],\n",
    "                svd_solver=params['PCA__svd_solver'],\n",
    "                tol=params.get('PCA__tol', None),                      # Include tolerance if provided\n",
    "                random_state=42\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Fit the pipeline on training data and transform test data\n",
    "        pca_pipeline.fit(Y_train)\n",
    "        transformed_train = pca_pipeline.transform(Y_train)\n",
    "        transformed_test = pca_pipeline.transform(Y_test)\n",
    "        \n",
    "        # Inverse transform the test data for reconstruction\n",
    "        reconstructed_test = pca_pipeline.named_steps['PCA'].inverse_transform(transformed_test)\n",
    "        \n",
    "        # Calculate reconstruction error on test data\n",
    "        error = mean_squared_error(Y_test, reconstructed_test)\n",
    "        print(f\"Reconstruction Error (Test) for combination {current_iteration}/{total_combinations}: {error:.4f}\")\n",
    "        \n",
    "        # Update the best parameters if test reconstruction error improves\n",
    "        if error < best_error:\n",
    "            best_error = error\n",
    "            best_params = params\n",
    "            print(f\"✅ New Best Reconstruction Error: {error:.4f} with parameters {params}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Combination {current_iteration}/{total_combinations} failed with error: {e}\")\n",
    "\n",
    "# Print the final best parameters and reconstruction error\n",
    "print(\"\\nSearch Completed!\")\n",
    "print(\"Best Parameters for PCA:\", best_params)\n",
    "print(\"Lowest Reconstruction Error (Test):\", best_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29fdec",
   "metadata": {},
   "source": [
    "#### Conclusions\n",
    "\n",
    "- **Reconstruction Error (459695.21)** PCA successfully reduced dimensionality without significant data loss.\n",
    "- **Tolerance(0.0001)** Ensures PCA converges with high precision by preventing premature stopping.\n",
    "- **PCA_svd_solver('auto')** The best solver is automatically choosen based on dataset size, balancing speed and accuracy.\n",
    "\n",
    "#### Summary\n",
    "The best parameters provide a good balance between efficiency and minimizing reconstruction error by ensuring optimal preservation of data structure and maintaining meaningful cluster separability in the reduced-dimensional space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5bdd5-a42c-43fb-8f15-606544bf67f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
